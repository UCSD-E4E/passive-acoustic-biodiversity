{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTS Pipeline Integration with UMAP, HDBSCAN\n",
    "Exploratory notebook for working on birdnet embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from annotation_post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingColumns = [str(i) for i in range(420)] + [\"UMAP_0\", \"UMAP_1\"]\n",
    "columnNames = [\"START\", \"END\"] + embeddingColumns\n",
    "path = './input/cosmos_embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv(\"./input/umap_cosmos_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_dfs:list[pd.DataFrame] = []\n",
    "automated_dfs.append(pd.read_csv(\"./cosmos_annotations/automated_cosmos_tweety_to_file.csv\"))\n",
    "automated_dfs.append(pd.read_csv(\"./cosmos_annotations/COSMOS_BirdNET-Lite_Labels_05Conf.csv\"))\n",
    "automated_dfs.append(pd.read_csv(\"./cosmos_annotations/COSMOS_BirdNET-Lite_Labels_100.csv\"))\n",
    "automated_dfs.append(pd.read_csv(\"./cosmos_annotations/COSMOS_BirdNET-Lite-Filename_Labels_05Conf.csv\"))\n",
    "automated_dfs.append(pd.read_csv(\"./cosmos_annotations/COSMOS_Microfaune-Filename_Labels_100.csv\"))\n",
    "print(automated_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for general embedding clustering\n",
    "# hdbscan_results = pd.read_csv(\"./ClusteringModels/umap_general.csv\")\n",
    "\n",
    "# Results for species-specific clustering\n",
    "hdbscan_all = dict()\n",
    "unique_species = embeddings_df[\"FILE SPECIES\"].unique()\n",
    "i = 0\n",
    "for a in [5, 10, 20, 50, 100, 200, 500]:\n",
    "    for b in [5, 10, 20, 50, 100, 200, 500]:\n",
    "        hdbscan_results = pd.DataFrame(columns=[\"FILE SPECIES\", \"PATH\"] + columnNames + [\"IN FILE\", \"LABELS\"])\n",
    "        j = 0\n",
    "        for species in unique_species:\n",
    "            species_result = pd.read_csv(f\"./ClusteringModels/umap_species_specific_hyper/{a}_{b}_{species}.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "            # Method 1: Simply filters out what was labeled as noise in recording\n",
    "            filter_1 = species_result[species_result[\"LABELS\"] == -1]\n",
    "            \n",
    "            # Method 2: Filters out noise and creates the filter by checking the n most frequent values of embedding labels (essentially to see most frequent bird labels, should be dominant)\n",
    "            n = 2\n",
    "            species_result = species_result[species_result[\"LABELS\"] != -1]\n",
    "            max_nums = species_result[\"LABELS\"].value_counts()[:n].index.tolist() # picking n most frequent values\n",
    "            filter_2 = species_result[~species_result[\"LABELS\"].isin(max_nums)]\n",
    "            \n",
    "            # filter = filter_1\n",
    "            filter = pd.concat([filter_1, filter_2], axis=0)\n",
    "            \n",
    "            hdbscan_results = pd.concat([hdbscan_results, filter], axis=0)\n",
    "            j += 1\n",
    "            print(f\"Done with {j} of {len(unique_species)}\")\n",
    "        hdbscan_all[a,b] = hdbscan_results.reset_index(drop = True)\n",
    "        i += 1\n",
    "        print(f\"Done with {i} iterations of hyperparameters\")\n",
    "        \n",
    "\n",
    "hdbscan_all[5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_embeddings = hdbscan_all\n",
    "print(\"Created filter\")\n",
    "\n",
    "filtered_embeddings[5,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "def split_annotations(df: pd.DataFrame):\n",
    "    all_split_ann = pd.DataFrame(columns = df.columns)\n",
    "    for i in range(df.shape[0]):\n",
    "        x = df.iloc[i]\n",
    "        startsends = np.linspace(3.0 * (int(x[\"OFFSET\"] / 3)), 3.0 * (int((x[\"OFFSET\"] + x[\"DURATION\"])/ 3) + 1), int((x[\"OFFSET\"] + x[\"DURATION\"])/ 3) - int(x[\"OFFSET\"] / 3) + 2)\n",
    "        starts = startsends[:-1]\n",
    "        starts[0] = x[\"OFFSET\"]\n",
    "        ends = startsends[1:]\n",
    "        ends[-1] = x[\"OFFSET\"] + x[\"DURATION\"]\n",
    "        split_ann = pd.DataFrame(columns = x.index)\n",
    "        for i in range(len(starts)):\n",
    "            new_x = pd.DataFrame(x.copy()).T\n",
    "            new_x[\"OFFSET\"] = starts[i]\n",
    "            new_x[\"DURATION\"] = ends[i] - starts[i]\n",
    "            if np.isclose(new_x[\"DURATION\"], 0):\n",
    "                continue\n",
    "            split_ann = pd.concat([split_ann, new_x])\n",
    "        all_split_ann = pd.concat([all_split_ann, split_ann])\n",
    "        global count1\n",
    "        count1 += 1\n",
    "        print(f\"Completed {count1} annotations\")\n",
    "    return all_split_ann.reset_index(drop = True)\n",
    "\n",
    "count2 = 0\n",
    "def create_annotation_filter(x: pd.Series, filter: pd.DataFrame) -> pd.DataFrame:\n",
    "    filter_x = filter[filter[\"IN FILE\"].str.startswith(x[\"IN FILE\"].split(\".mp3\")[0])]\n",
    "    starts = filter_x[\"START\"].to_numpy()\n",
    "    ends = filter_x[\"END\"].to_numpy()\n",
    "    close_starts = np.isclose(starts, x[\"OFFSET\"]).sum()\n",
    "    close_ends = np.isclose(ends, x[\"OFFSET\"] + x[\"DURATION\"]).sum()\n",
    "    middle1 = starts < x[\"OFFSET\"]\n",
    "    middle2 = ends > x[\"OFFSET\"] + x[\"DURATION\"]\n",
    "    middle = (middle1*middle2).sum()\n",
    "    if (close_starts + close_ends + middle) > 0:\n",
    "        x[\"FILTERED\"] = True\n",
    "    else:\n",
    "        x[\"FILTERED\"] = False\n",
    "    global count2\n",
    "    count2 += 1\n",
    "    print(f\"Completed {count2} annotations\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_dfs_split = [split_annotations(df) for df in automated_dfs]\n",
    "\n",
    "print(automated_dfs_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_filtered_all = dict()\n",
    "for a in [50, 500]:\n",
    "    for b in [20, 200]:\n",
    "        automated_dfs_filtered = [df.apply(lambda x: create_annotation_filter(x, filtered_embeddings[a, b]), axis = 1) for df in automated_dfs_split]\n",
    "        automated_dfs_filtered = [df[~df[\"FILTERED\"]] for df in automated_dfs_filtered]\n",
    "        automated_filtered_all[a,b] = automated_dfs_filtered\n",
    "print(automated_filtered_all[5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([df.shape[0] for df in automated_dfs])\n",
    "print([df.shape[0] for df in automated_dfs_split])\n",
    "print([df.shape[0] for df in automated_filtered_all[5,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics_1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_df = pd.read_csv(\"cosmos_annotations/cosmos_labeled_data_files_added.csv\")\n",
    "manual_df[\"IN FILE\"] = manual_df[\"IN FILE\"].apply(lambda x: \" \".join(x.split(\"_\")))\n",
    "manual_df[\"FOLDER\"] = \"./cosmos_annotations/\"\n",
    "manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "min_cluster_size = 50\n",
    "min_samples = 20\n",
    "clip_stats_original = [clip_statistics(df, manual_df, \"general\") for df in automated_dfs]\n",
    "clip_stats_filtered = [clip_statistics(df, manual_df, \"general\") for df in automated_filtered_all[5,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats_original = [class_statistics(stats) for stats in clip_stats_original]\n",
    "class_stats_filtered = [class_statistics(stats) for stats in clip_stats_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_stats_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_class_stats = [pd.concat([class_stats_original[i].assign(MODEL = \"original\"), class_stats_filtered[i].assign(MODEL = \"filtered\")]) for i in range(len(class_stats_original))]\n",
    "all_class_stats = [df[df[\"MANUAL ID\"] != \"Lipaugus vociferans\"] for df in all_class_stats]\n",
    "all_class_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting each model separately\n",
    "figure, axes = plt.subplots(1, len(class_stats_original), figsize = (30, 5), sharex = False, sharey = True)\n",
    "\n",
    "x = 0\n",
    "model_list = [\"Tweety to File\", \"BirdNET-Lite Labels 05Conf\", \"BirdNET-Lite Labels 100\", \"BirdNET-Lite to Filename\", \"Microfaune to Filename\"]\n",
    "\n",
    "for model in model_list:\n",
    "    plot = sns.barplot(ax = axes[x], data = all_class_stats[x], x = \"MANUAL ID\", y = \"PRECISION\", hue = \"MODEL\")\n",
    "    for label in plot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    plot.set(title = model)\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, len(class_stats_original), figsize = (30, 5), sharex = False, sharey = True)\n",
    "\n",
    "x = 0\n",
    "\n",
    "for model in model_list:\n",
    "    plot = sns.barplot(ax = axes[x], data = all_class_stats[x], x = \"MANUAL ID\", y = \"RECALL\", hue = \"MODEL\")\n",
    "    for label in plot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    plot.set(title = model)\n",
    "    x += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('species-id')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50f29603170da42ff0485de93cf455ea9e81b4c54cac7530327aeacc7af59708"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
