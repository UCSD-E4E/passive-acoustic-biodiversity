{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406a1103",
   "metadata": {},
   "source": [
    "# Articles\n",
    "https://keras.io/guides/transfer_learning/\n",
    "https://keras.io/guides/training_with_built_in_methods/\n",
    "https://www.youtube.com/watch?v=4umFSRPx-94&ab_channel=DigitalSreeni\n",
    "https://www.tensorflow.org/guide/saved_model\n",
    "https://github.com/UCSD-E4E/PyHa/blob/Microfaune_Retraining/Microfaune_Retraining-Copy1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f4dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bde0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "#add comments\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import librosa\n",
    "from librosa import display\n",
    "from microfaune.audio import wav2spc, create_spec, load_wav\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import IPython.display as ipd\n",
    "\n",
    "#from TweetyNetModel import TweetyNetModel  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2994afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac2e0f",
   "metadata": {},
   "source": [
    "## Preprocess and load data\n",
    "According to `config.py` and `checkpoints/README.md` for V2.1:  \n",
    "- Model training input size: 144000 = 48000 x 3 = sample rate x num chunks  \n",
    "- Model training output size: 2434 = 2424 bird classes + 10 non-event classes\n",
    "- Visualize using [Netron](https://netron.app/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336323c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "fineTuning = False\n",
    "#needs at least 80 for mel spectrograms ## may be able to do a little less, but must be greater than 60\n",
    "n_mels=72 # The closest we can get tmeporally is 72 with an output of 432 : i think it depends on whats good\n",
    "#this number should be proportional to the length of the videos. \n",
    "datasets_dir = \"../passive-acoustic-biodiversity/TweetyNET/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d25f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mel_spectrogram(spec, uid):\n",
    "    fig, ax = plt.subplots()\n",
    "    M_db_bird = librosa.power_to_db(spec, ref=np.max)\n",
    "    img = librosa.display.specshow(M_db_bird, y_axis='mel', x_axis='time', ax=ax)\n",
    "    ax.set(title=uid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3f2c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def splitSignal(sig, rate, overlap, seconds=3.0, minlen=1.5):\n",
    "\n",
    "    # Split signal with overlap\n",
    "    sig_splits = []\n",
    "    for i in range(0, len(sig), int((seconds - overlap) * rate)):\n",
    "        split = sig[i:i + int(seconds * rate)]\n",
    "\n",
    "        # End of signal?\n",
    "        if len(split) < int(minlen * rate):\n",
    "            break\n",
    "        \n",
    "        # Signal chunk too short? Fill with zeros.\n",
    "        if len(split) < int(rate * seconds):\n",
    "            temp = np.zeros((int(rate * seconds)))\n",
    "            temp[:len(split)] = split\n",
    "            split = temp\n",
    "        \n",
    "        sig_splits.append(split)\n",
    "\n",
    "    return sig_splits\n",
    "\n",
    "def readAudioData(path, overlap, sample_rate=48000):\n",
    "\n",
    "    #print('READING AUDIO DATA...', end=' ', flush=True)\n",
    "\n",
    "    # Open file with librosa (uses ffmpeg or libav)\n",
    "    try:\n",
    "        sig, rate = librosa.load(path, sr=sample_rate, mono=True, res_type='kaiser_fast')\n",
    "        clip_length = librosa.get_duration(y=sig, sr=rate)\n",
    "    except:\n",
    "        return 0\n",
    "    # Split audio into 3-second chunks\n",
    "    chunks = splitSignal(sig, rate, overlap)\n",
    "\n",
    "    #rint('DONE! READ', str(len(chunks)), 'CHUNKS.')\n",
    "\n",
    "    return chunks, clip_length\n",
    "\n",
    "def load_dataset(data_path, use_dump=True):\n",
    "    mel_dump_file = os.path.join(data_path, \"mel_dataset.pkl\")\n",
    "    if os.path.exists(mel_dump_file) and use_dump:\n",
    "        with open(mel_dump_file, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "    else:\n",
    "        dataset = compute_feature(data_path)\n",
    "        with open(mel_dump_file, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    inds = [i for i, x in enumerate(dataset[\"X\"])]\n",
    "    Y = np.array([int(dataset[\"Y\"][i]) for i in inds])\n",
    "    uids = [dataset[\"uids\"][i] for i in inds]\n",
    "    X = np.array(dataset[\"X\"]).transpose()\n",
    "    del dataset\n",
    "    return X, Y, uids\n",
    "\n",
    "def compute_feature(data_path):\n",
    "    print(f\"Compute features for dataset {os.path.basename(data_path)}\")\n",
    "    labels_file = os.path.join(data_path, \"labels.csv\")\n",
    "    print(labels_file)\n",
    "    if os.path.exists(labels_file):\n",
    "        with open(labels_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            labels = {}\n",
    "            next(reader)  # pass fields names\n",
    "            for name, _, y in reader:\n",
    "                labels[name] = y\n",
    "    else:\n",
    "        print(\"Warning: no label file detected.\")\n",
    "        wav_files = glob(os.path.join(data_path, \"wav/*.wav\"))\n",
    "        labels = {os.path.basename(f)[:-4]: None for f in wav_files}\n",
    "    i = 1\n",
    "    X = []\n",
    "    Y = []\n",
    "    uids = []\n",
    "    for file_id, y in labels.items():\n",
    "        print(y)\n",
    "        print(f\"{i:04d}/{len(labels)}: {file_id:20s}\", end=\"\\r\")\n",
    "        spc = wav2spc(os.path.join(data_path, \"wav\", f\"{file_id}.wav\"), n_mels=n_mels)\n",
    "        chunks, clip_length = readAudioData(os.path.join(data_path, \"wav\", f\"{file_id}.wav\"), 0, sample_rate=48000)\n",
    "        for c in range(len(chunks)):\n",
    "            X.append(chunks[c])\n",
    "            Y.append(y)\n",
    "            uids.append(file_id)\n",
    "           \n",
    "    return {\"uids\": uids, \"X\": X, \"Y\": Y}\n",
    "\n",
    "def split_dataset(X, Y, test_size=0.2, random_state=0):\n",
    "    split_generator = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    ind_train, ind_test = next(split_generator.split(X, Y))\n",
    "    X_train, X_test = X[ind_train, :, :], X[ind_test, :, :]\n",
    "    Y_train, Y_test = Y[ind_train], Y[ind_test]\n",
    "    return ind_train, ind_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48726ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 25.8 GiB for an array with shape (24046, 144000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4bc4de591256>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatasets_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./Data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muids0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ff1010bird_wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muids1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"warblrb10k_public_wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-0e49d0042ded>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(data_path, use_dump)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0muids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"uids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 25.8 GiB for an array with shape (24046, 144000) and data type float64"
     ]
    }
   ],
   "source": [
    "datasets_dir = \"./Data\"\n",
    "X0, Y0, uids0 = load_dataset(os.path.join(datasets_dir, \"ff1010bird_wav\"))\n",
    "#X1, Y1, uids1 = load_dataset(os.path.join(datasets_dir, \"warblrb10k_public_wav\"))\n",
    "print(X0.shape, X1.shape)\n",
    "print(Y0.shape, Y1.shape)\n",
    "print(len(uids0), len(uids1))\n",
    "\n",
    "X = np.concatenate([X0, X1]).astype(np.float32)/255\n",
    "Y = np.concatenate([Y0, Y1])\n",
    "uids = np.concatenate([uids0, uids1])\n",
    "print(X.shape, Y.shape, uids.shape)\n",
    "del X0, Y0, uids0, X1, Y1, uids1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_val, ind_test = split_dataset(X, Y)\n",
    "ind_train, ind_val = split_dataset(X[ind_train_val, :, :, np.newaxis], Y[ind_train_val], test_size=0.1)\n",
    "X_train, X_test, X_val = X[ind_train, :, :, np.newaxis], X[ind_test, :, :, np.newaxis], X[ind_val, :, :, np.newaxis]\n",
    "Y_train, Y_test, Y_val = Y[ind_train], Y[ind_test], Y[ind_val]\n",
    "uids_train, uids_test, uids_val = uids[ind_train], uids[ind_test], uids[ind_val]\n",
    "del X, Y\n",
    "\n",
    "print(\"Training set: \", Counter(Y_train))\n",
    "print(\"Test set: \", Counter(Y_test))\n",
    "print(\"Validation set: \", Counter(Y_val))\n",
    "#very imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8228002",
   "metadata": {},
   "source": [
    "## Load in BirdNET-Analyzer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9348a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to model folder, should have assets/variables/frozen pb graphs\n",
    "path_to_saved_model = \"./checkpoints/V2.1/BirdNET_GLOBAL_2K_V2.1_Model\"\n",
    "model = tf.keras.models.load_model(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b611ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb3693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = model.layers[-2].output\n",
    "#o = tf.keras.layers.Dense(len(species_of_interests), activation='sigmoid', name='its_new_lmao')(x)\n",
    "o = tf.keras.layers.Dense(2, activation='sigmoid', name='its_new_lmao')(x)\n",
    "\n",
    "model3 = Model(inputs=model.input, outputs=o)\n",
    "#model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875da784",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(species_of_interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extended = model3\n",
    "model_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f39166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extended.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.set_soft_device_placement(True)\n",
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74fc1e",
   "metadata": {},
   "source": [
    "## Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfeedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, Y, batch_size=32, output_size=10):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n = len(Y)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        print(\"RUNNINGS\", int(np.floor(self.n)/self.batch_size))\n",
    "        return int(np.floor(self.n)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_inds = self.inds[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "        #print(index, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "        if (tf.reduce_max(tensor) == tf.reduce_min(tensor)):\n",
    "            try:\n",
    "                print(\"REDO\")\n",
    "                print(index, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "                batch_inds = self.inds[self.batch_size*(index+1):self.batch_size*(index+ 2)]\n",
    "                tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "                print(index+1, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "            except:\n",
    "                batch_inds = self.inds[self.batch_size*(index-1):self.batch_size*(index)]\n",
    "                tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "\n",
    "        tensor = tf.math.divide(\n",
    "           tf.math.subtract(\n",
    "              tensor, \n",
    "              tf.reduce_min(tensor)\n",
    "           ), \n",
    "           tf.math.subtract(\n",
    "              tf.reduce_max(tensor), \n",
    "              tf.reduce_min(tensor)\n",
    "           )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(batch_inds)\n",
    "        self.counter += self.batch_size\n",
    "        if self.counter >= self.n:\n",
    "            self.shuffle()\n",
    "            \n",
    "        #TODO: FIX SO BATCH IS MORE THAN 1\n",
    "        raw_labels = np.array([self.Y[batch_inds][0]])\n",
    "        formatted_labels = np.zeros(self.output_size)\n",
    "        formatted_labels[self.Y[batch_inds][0]] = 1\n",
    "        fprmatted_labels = np.array([formatted_labels])\n",
    "        #print(formatted_labels.shape)\n",
    "        #print(np.array([formatted_labels]).shape)\n",
    "        #print(self.X[batch_inds, ...][0].shape)\n",
    "\n",
    "        \n",
    "        #print(tf.convert_to_tensor(self.X[batch_inds, ...][0]), tf.convert_to_tensor(np.array([formatted_labels])))\n",
    "        return tensor, tf.convert_to_tensor(np.array([formatted_labels]))\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.inds = np.random.permutation(self.n)\n",
    "        self.counter = 0\n",
    "# if train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(species_of_interests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22a857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# microfaune training script\n",
    "\n",
    "if train:\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model_extended.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy', keras.metrics.FalseNegatives()])\n",
    "\n",
    "    alpha = 0.5\n",
    "    batch_size = 1\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    data_generator = DataGenerator(X_train, Y_train, batch_size, output_size=2)\n",
    "    \n",
    "    micro_callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=1e-5),\n",
    "        #keras.callbacks.ModelCheckpoint('microfaune-' + date_str +'-{epoch:02d}.h5',\n",
    "        #                          save_weights_only=False)\n",
    "    ]\n",
    "    \n",
    "    #validation_data=(X_test, Y_test),\n",
    "    \n",
    "    history = model_extended.fit(data_generator, epochs=4,batch_size=1,workers=6,\n",
    "                                  #validation_data=(X_test, Y_test), class_weight={0: alpha, 1: 1-alpha},callbacks=micro_callbacks,\n",
    "                                   verbose=2)\n",
    "    \n",
    "    #model.save(f\"model-{date_str}\")\n",
    "    #model.save_weights(f\"model_weights-{date_str}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_generator = DataGenerator(X_train, Y_train, batch_size, output_size=len(species_of_interests))\n",
    "\n",
    "sub_data = data_generator.__getitem__(20585)\n",
    "print(\"=================\")\n",
    "print(sub_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66f5d5",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "class ValidDataset(keras.utils.Sequence):\n",
    "    def __init__(self, X, Y, batch_size=32, output_size=10):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        #self.UIDs = UIDs\n",
    "        self.n = len(Y)\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.shuffle()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        print(\"RUNNINGS\", int(np.floor(self.n)/self.batch_size))\n",
    "        return int(np.floor(self.n)/self.batch_size)\n",
    "    \n",
    "    def len_of_labels(self):\n",
    "        return self.output_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_inds = self.inds[self.batch_size*index:self.batch_size*(index+1)]\n",
    "        tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "        #print(index, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "        if (tf.reduce_max(tensor) == tf.reduce_min(tensor)):\n",
    "            try:\n",
    "                print(\"REDO\")\n",
    "                print(index, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "                batch_inds = self.inds[self.batch_size*(index+1):self.batch_size*(index+ 2)]\n",
    "                tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "                print(index+1, tf.reduce_max(tensor), tf.reduce_min(tensor), tf.reduce_max(tensor) == tf.reduce_min(tensor))\n",
    "            except:\n",
    "                batch_inds = self.inds[self.batch_size*(index-1):self.batch_size*(index)]\n",
    "                tensor = tf.convert_to_tensor(self.X[batch_inds, ...][0])\n",
    "\n",
    "        tensor = tf.math.divide(\n",
    "           tf.math.subtract(\n",
    "              tensor, \n",
    "              tf.reduce_min(tensor)\n",
    "           ), \n",
    "           tf.math.subtract(\n",
    "              tf.reduce_max(tensor), \n",
    "              tf.reduce_min(tensor)\n",
    "           )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(batch_inds)\n",
    "        self.counter += self.batch_size\n",
    "        if self.counter >= self.n:\n",
    "            self.shuffle()\n",
    "            \n",
    "        #TODO: FIX SO BATCH IS MORE THAN 1\n",
    "        raw_labels = np.array([self.Y[batch_inds][0]])\n",
    "        formatted_labels = np.zeros(self.output_size)\n",
    "        formatted_labels[self.Y[batch_inds][0]] = 1\n",
    "        fprmatted_labels = np.array([formatted_labels])\n",
    "        #print(formatted_labels.shape)\n",
    "        #print(np.array([formatted_labels]).shape)\n",
    "        #print(self.X[batch_inds, ...][0].shape)\n",
    "\n",
    "        \n",
    "        #print(tf.convert_to_tensor(self.X[batch_inds, ...][0]), tf.convert_to_tensor(np.array([formatted_labels])))\n",
    "        return tensor, tf.convert_to_tensor(np.array([formatted_labels]))\n",
    "    \n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.inds = np.random.permutation(self.n)\n",
    "        self.counter = 0\n",
    "# if train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acb335",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset = ValidDataset(X_test, Y_test, 1, output_size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " label_df = pd.DataFrame(columns=range(validate_dataset.len_of_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ac54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label_df_one_hot_encoding\n",
    "label_df = pd.DataFrame(columns=range(validate_dataset.len_of_labels()))\n",
    "scores_df = pd.DataFrame(columns=range(validate_dataset.len_of_labels()))\n",
    "preds_df = pd.DataFrame(columns=range(validate_dataset.len_of_labels()))\n",
    "for i in range(len(validate_dataset)):\n",
    "    #print(i)\n",
    "    predictions = model_extended.predict(\n",
    "        validate_dataset.__getitem__(i)[0],\n",
    "        batch_size=None,\n",
    "        verbose='1',\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=6,\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    label = predictions.argmax()\n",
    "    label_arr = np.zeros(validate_dataset.len_of_labels())\n",
    "    label_arr[label] = 1\n",
    "    \n",
    "    preds_df = preds_df.append(pd.DataFrame(np.array([label_arr])))\n",
    "    scores_df = scores_df.append(pd.DataFrame(predictions))\n",
    "    label_df = label_df.append(pd.DataFrame(validate_dataset.__getitem__(i)[1].numpy()))\n",
    "    \n",
    "preds_df.columns = np.append(species_of_interests, -1)\n",
    "scores_df.columns = np.append(species_of_interests, -1)\n",
    "label_df.columns = np.append(species_of_interests, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(label_df, preds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb96c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for species in species_of_interests:\n",
    "    try:\n",
    "        fpr, tpr, thresh = roc_curve(label_df[species],  scores_df[species])\n",
    "        auc = roc_auc_score(label_df[species],  preds_df[species])\n",
    "        plt.plot(fpr,tpr,label=\"AUC \" + species + \" \"+str(auc))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.title('Classwise ROC Curves')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23915f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(preds_df.apply(lambda x: sum(x), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401570cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb6218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70582fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
