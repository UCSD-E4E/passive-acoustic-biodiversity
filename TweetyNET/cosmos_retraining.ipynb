{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a67f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#add comments\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from network import TweetyNet\n",
    "import librosa\n",
    "from librosa import display\n",
    "from microfaune.audio import wav2spc, create_spec, load_wav\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from CustomAudioDataset import CustomAudioDataset\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import IPython.display as ipd\n",
    "\n",
    "from TweetyNetModel import TweetyNetModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811b347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "fineTuning = False\n",
    "#needs at least 80 for mel spectrograms ## may be able to do a little less, but must be greater than 60\n",
    "n_mels=72 # The closest we can get tmeporally is 72 with an output of 432 : i think it depends on whats good\n",
    "#this number should be proportional to the length of the videos. \n",
    "datasets_dir = \"C:/Users/Siloux/Desktop/E4E/passive-acoustic-biodiversity/TweetyNET/cosmos_data/cosmos_random_sample_processing\"\n",
    "this_is_new_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d35705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#TODO Add mutliple species support for ROC curves \n",
    "def annotation_chunker_no_duplicates(kaleidoscope_df, chunk_length, include_no_bird=False, bird=None):\n",
    "    \"\"\"\n",
    "    Function that converts a Kaleidoscope-formatted Dataframe containing \n",
    "    annotations to uniform chunks of chunk_length. If there\n",
    "    are mutliple bird species in the same clip, this function creates chunks\n",
    "    for the more confident bird species.\n",
    "\n",
    "    Note: if all or part of an annotation covers the last < chunk_length\n",
    "    seconds of a clip it will be ignored. If two annotations overlap in \n",
    "    the same 3 second chunk, both are represented in that chunk\n",
    "\n",
    "    Args:\n",
    "        kaleidoscope_df (Dataframe)\n",
    "            - Dataframe of annotations in kaleidoscope format\n",
    "\n",
    "        chunk_length (int)\n",
    "            - duration to set all annotation chunks\n",
    "    Returns:\n",
    "        Dataframe of labels with chunk_length duration \n",
    "        (elements in \"OFFSET\" are divisible by chunk_length).\n",
    "    \"\"\"\n",
    "\n",
    "    #Init list of clips to cycle through and output dataframe\n",
    "    #kaleidoscope_df[\"FILEPATH\"] =  kaleidoscope_df[\"FOLDER\"] + kaleidoscope_df[\"IN FILE\"] \n",
    "    kaleidoscope_df['FILEPATH'] = kaleidoscope_df.loc[:,['FOLDER','IN FILE']].sum(axis=1)\n",
    "    clips = kaleidoscope_df[\"FILEPATH\"].unique()\n",
    "    df_columns = {'FOLDER': 'str', 'IN FILE' :'str', 'CLIP LENGTH' : 'float64', 'CHANNEL' : 'int64', 'OFFSET' : 'float64',\n",
    "                'DURATION' : 'float64', 'SAMPLE RATE' : 'int64','MANUAL ID' : 'str'}\n",
    "    output_df = pd.DataFrame({c: pd.Series(dtype=t) for c, t in df_columns.items()})\n",
    "    \n",
    "    # going through each clip\n",
    "    for clip in clips:\n",
    "        clip_df = kaleidoscope_df[kaleidoscope_df[\"FILEPATH\"] == clip]\n",
    "        path = clip_df[\"FOLDER\"].unique()[0]\n",
    "        file = clip_df[\"IN FILE\"].unique()[0]\n",
    "        birds = clip_df[\"MANUAL ID\"].unique()\n",
    "        sr = clip_df[\"SAMPLE RATE\"].unique()[0]\n",
    "        clip_len = clip_df[\"CLIP LENGTH\"].unique()[0]\n",
    "\n",
    "        # quick data sanitization to remove very short clips\n",
    "        # do not consider any chunk that is less than chunk_length\n",
    "        if clip_len < chunk_length:\n",
    "            continue\n",
    "        potential_annotation_count = int(clip_len)//int(chunk_length)\n",
    "\n",
    "        # going through each species that was ID'ed in the clip\n",
    "        arr_len = int(clip_len*1000)\n",
    "        species_df = clip_df#[clip_df[\"MANUAL ID\"] == bird]\n",
    "        human_arr = np.zeros((arr_len))\n",
    "        # looping through each annotation\n",
    "        #print(\"========================================\")\n",
    "        for annotation in species_df.index:\n",
    "            #print(species_df[\"OFFSET\"][annotation])\n",
    "            minval = int(round(species_df[\"OFFSET\"][annotation] * 1000, 0))\n",
    "            # Determining the end of a human label\n",
    "            maxval = int(\n",
    "                round(\n",
    "                    (species_df[\"OFFSET\"][annotation] +\n",
    "                        species_df[\"DURATION\"][annotation]) *\n",
    "                    1000,\n",
    "                    0))\n",
    "            # Placing the label relative to the clip\n",
    "            human_arr[minval:maxval] = 1\n",
    "        # performing the chunk isolation technique on the human array\n",
    "\n",
    "        for index in range(potential_annotation_count):\n",
    "            #print(\"=======================\")\n",
    "            #print(\"-----------------------------------------\")\n",
    "            #print(index)\n",
    "            chunk_start = index * (chunk_length*1000)\n",
    "            chunk_end = min((index+1)*chunk_length*1000,arr_len)\n",
    "            chunk = human_arr[int(chunk_start):int(chunk_end)]\n",
    "            if max(chunk) >= 0.5:\n",
    "                #Get row data\n",
    "                row = pd.DataFrame(index = [0])\n",
    "                annotation_start = chunk_start / 1000\n",
    "\n",
    "                #Handle birdnet output edge case\n",
    "                #print(\"-------------------------------------------\")\n",
    "                #print(sum(clip_df[\"DURATION\"] == 3))\n",
    "                #print(sum(clip_df[\"DURATION\"] == 3)/clip_df.shape[0])\n",
    "                #print(\"-------------------------------------------\")\n",
    "                if(sum(clip_df[\"DURATION\"] == 3)/clip_df.shape[0] == 1):\n",
    "                    #print(\"Processing here duration\")\n",
    "                    overlap = (clip_df[\"OFFSET\"]+0.5 >= (annotation_start)) & (clip_df[\"OFFSET\"]-0.5 <= (annotation_start))\n",
    "                    annotation_df = clip_df[overlap]\n",
    "                    #print(annotation_start, np.array(clip_df[\"OFFSET\"]), overlap)\n",
    "                    #print(annotation_df)\n",
    "                else:\n",
    "                    #print(\"Processing here\")\n",
    "                    overlap = is_overlap(clip_df[\"OFFSET\"], clip_df[\"OFFSET\"] + clip_df[\"DURATION\"], annotation_start, annotation_start + chunk_length)\n",
    "                    #print(overlap)\n",
    "                    annotation_df = clip_df[overlap]\n",
    "                    #print(annotation_df)\n",
    "                \n",
    "                #updating the dictionary\n",
    "                if ('CONFIDENCE' in clip_df.columns):\n",
    "                    annotation_df = annotation_df.sort_values(by=\"CONFIDENCE\", ascending=False)\n",
    "                    row[\"CONFIDENCE\"] = annotation_df.iloc[0][\"CONFIDENCE\"]\n",
    "                else:\n",
    "                    #The case of manual id, or there is an annotation with no known confidence\n",
    "                    row[\"CONFIDENCE\"] = 1\n",
    "                row[\"FOLDER\"] = path\n",
    "                row[\"IN FILE\"] = file\n",
    "                row[\"CLIP LENGTH\"] = clip_len\n",
    "                row[\"OFFSET\"] = annotation_start\n",
    "                row[\"DURATION\"] = chunk_length\n",
    "                row[\"SAMPLE RATE\"] = sr\n",
    "                row[\"MANUAL ID\"] = annotation_df.iloc[0][\"MANUAL ID\"] \n",
    "                row[\"CHANNEL\"] = 0\n",
    "                output_df = pd.concat([output_df,row], ignore_index=True)\n",
    "            elif(include_no_bird):\n",
    "                #print(max(chunk))\n",
    "                #Get row data\n",
    "                row = pd.DataFrame(index = [0])\n",
    "                annotation_start = chunk_start / 1000\n",
    "\n",
    "                #updating the dictionary\n",
    "                row[\"CONFIDENCE\"] = 0\n",
    "                row[\"FOLDER\"] = path\n",
    "                row[\"IN FILE\"] = file\n",
    "                row[\"CLIP LENGTH\"] = clip_len\n",
    "                row[\"OFFSET\"] = annotation_start\n",
    "                row[\"DURATION\"] = chunk_length\n",
    "                row[\"SAMPLE RATE\"] = sr\n",
    "                row[\"MANUAL ID\"] = \"no bird\"\n",
    "                row[\"CHANNEL\"] = 0\n",
    "                output_df = pd.concat([output_df,row], ignore_index=True)\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n",
    "def is_overlap(offset_df, end_df, chunk_start, chunk_end):\n",
    "    is_both_before = (chunk_end < offset_df) & (chunk_start < offset_df)\n",
    "    is_both_after = (end_df < chunk_end) & (end_df < chunk_start)\n",
    "    return (~is_both_before) & (~is_both_after)\n",
    "    \n",
    "    \n",
    "    interval = pd.Interval(left=offset_df, right=end_df)\n",
    "    print(interval)\n",
    "\n",
    "\n",
    "def split_save_files(dataframe_of_data):\n",
    "    chunked_df = annotation_chunker_no_duplicates(dataframe_of_data, 3, include_no_bird=True)\n",
    "    \n",
    "    \n",
    "    chunked_df.apply()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/Siloux/Desktop/E4E/passive-acoustic-biodiversity/TweetyNET/cosmos_data/cosmos_random_sample_processing/automated_cosmos_tweety_to_birdnet.csv\")\n",
    "chunked_df = annotation_chunker_no_duplicates(data, 3, include_no_bird=True)\n",
    "chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99603df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df.iloc[0][\"IN FILE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub\n",
    "pydub.AudioSegment.ffmpeg = \"C:/Users/Siloux/Downloads/ffmpeg-master-latest-win64-gpl/ffmpeg-master-latest-win64-gpl/bin/ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49864274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "AudioSegment.converter = \"C:/Users/Siloux/Downloads/ffmpeg-master-latest-win64-gpl/ffmpeg-master-latest-win64-gpl/bin/ffmpeg.exe\"\n",
    "import os\n",
    "\n",
    "path_to_audio = \"C:/Users/Siloux/Desktop/E4E/passive-acoustic-biodiversity/TweetyNET/cosmos_data/cosmos_random_sample_processing\"\n",
    "data_path = \"C:/Users/Siloux/Desktop/E4E/passive-acoustic-biodiversity/TweetyNET/cosmos_data/cosmos_random_sample_processing_split\"\n",
    "new_folder = data_path\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(path_to_audio) if isfile(join(path_to_audio, f))]\n",
    "chunked_df = chunked_df[chunked_df[\"IN FILE\"].isin(onlyfiles)]\n",
    "\n",
    "\n",
    "chunked_renamed_df = chunked_df.copy()\n",
    "count = 0\n",
    "for filename in np.unique(chunked_df[\"IN FILE\"]):\n",
    "    file_df = chunked_df[chunked_df[\"IN FILE\"] == filename]\n",
    "    file, file_type = filename.split(\".\")\n",
    "    for index,row in file_df.iterrows():\n",
    "        t1 = row[\"OFFSET\"] * 1000\n",
    "        # end time in milliseconds\n",
    "        t2 = (row[\"OFFSET\"] + 3) * 1000\n",
    "        new_name = file + \"_\" + str(int(row[\"OFFSET\"])) +\".\" + file_type\n",
    "        new_path = new_folder + \"/\" + new_name\n",
    "        \n",
    "        if(this_is_new_data):\n",
    "            newAudio = AudioSegment.from_mp3(path_to_audio + \"/\" + filename)\n",
    "            newAudio = newAudio[t1:t2]\n",
    "            newAudio.export(new_path, format=\"mp3\")\n",
    "\n",
    "        chunked_renamed_df.at[index, \"IN FILE\"] = new_name\n",
    "        chunked_renamed_df.at[index, \"FOLDER\"] = new_folder\n",
    "        count += 1\n",
    "        print(count, \"out of 6451\")\n",
    "\n",
    "        \n",
    "chunked_renamed_df = chunked_renamed_df[chunked_renamed_df[\"IN FILE\"] != \"TO DELETE\"]\n",
    "chunked_renamed_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ce294",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chunked_renamed_df.groupby([\"IN FILE\", \"OFFSET\"]).count().sort_values(by=\"MANUAL ID\", ascending=False)\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mel_spectrogram(spec, uid):\n",
    "    fig, ax = plt.subplots()\n",
    "    M_db_bird = librosa.power_to_db(spec, ref=np.max)\n",
    "    img = librosa.display.specshow(M_db_bird, y_axis='mel', x_axis='time', ax=ax)\n",
    "    ax.set(title=uid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, use_dump=True, csv=\"labels.csv\", kalediscope=False):\n",
    "    mel_dump_file = os.path.join(data_path, \"mel_dataset.pkl\")\n",
    "    if os.path.exists(mel_dump_file) and use_dump:\n",
    "        with open(mel_dump_file, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "    \n",
    "    elif kalediscope:\n",
    "        dataset = compute_feature_kaledoscope_labels(data_path, csv)\n",
    "        with open(mel_dump_file, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    else:\n",
    "        dataset = compute_feature(data_path, csv)\n",
    "        with open(mel_dump_file, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    inds = [i for i, x in enumerate(dataset[\"X\"]) if x.shape[1] == 431]\n",
    "    X = np.array([dataset[\"X\"][i].transpose() for i in inds])\n",
    "    Y = np.array([int(dataset[\"Y\"][i]) for i in inds])\n",
    "    uids = [dataset[\"uids\"][i] for i in inds]\n",
    "    return X, Y, uids\n",
    "\n",
    "def compute_feature(data_path, csv = \"labels.csv\"):\n",
    "    print(f\"Compute features for dataset {os.path.basename(data_path)}\")\n",
    "    labels_file = os.path.join(data_path, csv)\n",
    "    print(labels_file)\n",
    "    if os.path.exists(labels_file):\n",
    "        with open(labels_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            labels = {}\n",
    "            next(reader)  # pass fields names\n",
    "            for name, _, y in reader:\n",
    "                labels[name] = y\n",
    "    else:\n",
    "        print(\"Warning: no label file detected.\")\n",
    "        wav_files = glob(os.path.join(data_path, \"wav/*.wav\"))\n",
    "        labels = {os.path.basename(f)[:-4]: None for f in wav_files}\n",
    "    i = 1\n",
    "    X = []\n",
    "    Y = []\n",
    "    uids = []\n",
    "    for file_id, y in labels.items():\n",
    "        print(f\"{i:04d}/{len(labels)}: {file_id:20s}\", end=\"\\r\")\n",
    "        spc = wav2spc(os.path.join(data_path, \"wav\", f\"{file_id}.wav\"), n_mels=n_mels)\n",
    "        X.append(spc)\n",
    "        Y.append(y)\n",
    "        uids.append(file_id)\n",
    "        i += 1\n",
    "    return {\"uids\": uids, \"X\": X, \"Y\": Y}\n",
    "\n",
    "def compute_feature_df(data_path, df):\n",
    "    \n",
    "    i = 1\n",
    "    X = []\n",
    "    Y = []\n",
    "    uids = []\n",
    "    for file_id, y in labels.items():\n",
    "        print(f\"{i:04d}/{len(labels)}: {file_id:20s}\", end=\"\\r\")\n",
    "        spc = wav2spc(os.path.join(data_path, \"wav\", f\"{file_id}.wav\"), n_mels=n_mels)\n",
    "        X.append(spc)\n",
    "        Y.append(y)\n",
    "        uids.append(file_id)\n",
    "        i += 1\n",
    "    return {\"uids\": uids, \"X\": X, \"Y\": Y}\n",
    "\n",
    "def split_dataset(X, Y, test_size=0.2, random_state=0):\n",
    "    split_generator = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    ind_train, ind_test = next(split_generator.split(X, Y))\n",
    "    X_train, X_test = X[ind_train, :, :], X[ind_test, :, :]\n",
    "    Y_train, Y_test = Y[ind_train], Y[ind_test]\n",
    "    return ind_train, ind_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = list(np.unique(chunked_renamed_df[\"MANUAL ID\"]))\n",
    "chunked_renamed_df[\"y\"] = chunked_renamed_df[\"MANUAL ID\"].apply(lambda x: species.index(x))\n",
    "chunked_renamed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ef393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# You dont need the number of files in the folder, just iterate over them directly using:\n",
    "if (this_is_new_data):\n",
    "    for index, row in chunked_renamed_df.iterrows():\n",
    "        file = row[\"IN FILE\"]\n",
    "        folder = row[\"FOLDER\"]\n",
    "        #spliting the file into the name and the extension\n",
    "        name, ext = os.path.splitext(folder + '/' + file)\n",
    "        if ext == \".mp3\":\n",
    "            #os.remove(folder + '/' + file) \n",
    "\n",
    "            mp3_sound = AudioSegment.from_mp3(folder + '/' + file)\n",
    "            mp3_sound.export(\"{0}.wav\".format(name), format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a67e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "if(this_is_new_data):\n",
    "    # You dont need the number of files in the folder, just iterate over them directly using:\n",
    "    for index, row in chunked_renamed_df.iterrows():\n",
    "        file = row[\"IN FILE\"]\n",
    "        folder = row[\"FOLDER\"]\n",
    "        #spliting the file into the name and the extension\n",
    "        name, ext = os.path.splitext(file)\n",
    "        if ext == \".mp3\":\n",
    "            print(folder)\n",
    "            try:\n",
    "                os.remove(folder + \"/\" + file) \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14cfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_renamed_df[\"IN FILE\"][201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc41d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_renamed_df[\"IN FILE\"] = chunked_renamed_df[\"IN FILE\"].apply(lambda x: x.replace(\".mp3\", \".wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# You dont need the number of files in the folder, just iterate over them directly using:\n",
    "if(this_is_new_data):\n",
    "    for index, row in chunked_renamed_df.iterrows():\n",
    "        file = row[\"IN FILE\"]\n",
    "        folder = row[\"FOLDER\"]\n",
    "        sound = AudioSegment.from_wav(folder + \"/\" + file)\n",
    "        sound = sound.set_channels(1)\n",
    "        sound.export(folder + \"/\" + file, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/Siloux/Desktop/E4E/passive-acoustic-biodiversity/TweetyNET/cosmos_data/cosmos_random_sample_processing_split\"\n",
    "i = 1\n",
    "X = []\n",
    "Y = []\n",
    "uids = []\n",
    "for index, row in chunked_renamed_df.iterrows():\n",
    "    try:\n",
    "        print(f\"{i:04d}/{chunked_renamed_df.shape[0]}\", end=\"\\r\")\n",
    "        print(os.path.join(data_path, row[\"IN FILE\"]))\n",
    "        spc = wav2spc(os.path.join(data_path, row[\"IN FILE\"]), n_mels=n_mels)\n",
    "        X.append(spc)\n",
    "        Y.append(row[\"y\"])\n",
    "        uids.append(row[\"IN FILE\"])\n",
    "        i += 1\n",
    "    except:\n",
    "        continue\n",
    "dataset = {\"uids\": uids, \"X\": X, \"Y\": Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede139d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inds = [i for i, x in enumerate(dataset[\"X\"]) if x.shape[1] == 130]\n",
    "X = np.array([dataset[\"X\"][i].transpose() for i in inds])\n",
    "Y = np.array([int(dataset[\"Y\"][i]) for i in inds])\n",
    "uids = [dataset[\"uids\"][i] for i in inds]\n",
    "X, Y, uids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"X\"][0].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X0, Y0, uids0 = load_dataset(datasets_dir, csv=\"automated_cosmos_tweety_to_file.csv\")#os.path.join(datasets_dir, \"ff1010bird_wav\"))\n",
    "#print(X0)\n",
    "#uids0, X0, Y0 =  uids, X,Y#load_dataset(os.path.join(datasets_dir, \"warblrb10k_public_wav\"))\n",
    "\n",
    "#print(X0.shape)\n",
    "#print(Y0.shape)\n",
    "#print(len(uids0))\n",
    "\n",
    "#X = np.concatenate([X0]).astype(np.float32)/255\n",
    "#Y = np.concatenate([Y0])\n",
    "#uids = np.concatenate([uids0])\n",
    "#print(X.shape, Y.shape, uids.shape)\n",
    "#del X0, Y0, uids0, X1, Y1, uids1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c78b1",
   "metadata": {},
   "source": [
    "Actual work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(uids))\n",
    "uids = np.array(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f728fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_train_val, ind_test = split_dataset(X, Y)\n",
    "ind_train, ind_val = split_dataset(X[ind_train_val, :, :, np.newaxis], Y[ind_train_val], test_size=0.1)\n",
    "X_train, X_test, X_val = X[ind_train, :, :, np.newaxis], X[ind_test, :, :, np.newaxis], X[ind_val, :, :, np.newaxis]\n",
    "Y_train, Y_test, Y_val = Y[ind_train], Y[ind_test], Y[ind_val]\n",
    "uids_train, uids_test, uids_val = uids[ind_train], uids[ind_test], uids[ind_val]\n",
    "#del X, Y\n",
    "\n",
    "print(\"Training set: \", Counter(Y_train))\n",
    "print(\"Test set: \", Counter(Y_test))\n",
    "print(\"Validation set: \", Counter(Y_val))\n",
    "#very imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Counter(Y_train))\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc36f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "tweetynet = TweetyNetModel(len(Counter(Y_train)), (1, n_mels, 431), device, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c72742",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.type(torch.LongTensor)\n",
    "test_dataset = test_dataset.type(torch.LongTensor)\n",
    "val_dataset = val_dataset.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history, test_out, start_time, end_time = tweetynet.train_pipeline(train_dataset, val_dataset, test_dataset, \n",
    "                                                                   lr=.001, batch_size=6,epochs=5, save_me=True,\n",
    "                                                                   fine_tuning=False, finetune_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee344f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_renamed_df.to_csv(\"formated_tweetynet_ground_truth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354699a",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aee55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#should categorize this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f52da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, sr = librosa.load(os.path.join(datasets_dir, folder_name, \"wav\", uid+\".wav\"))\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13789159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-100,-2\n",
    "idx_here = -100\n",
    "x = X[idx_here]\n",
    "y = Y[idx_here]\n",
    "uid = uids[idx_here]\n",
    "folder_name = \"ff1010bird_wav\" if \"-\" not in uid else \"warblrb10k_public_wav\"\n",
    "sr = 44100\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mel_spectrogram(x.transpose(), uid)\n",
    "y, uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8dc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(os.path.join(datasets_dir, folder_name, \"wav\", uid+\".wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49719e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(a, sr=sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
